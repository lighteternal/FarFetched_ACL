{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-01T20:28:25.915359Z",
     "start_time": "2022-02-01T20:28:07.440604Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/earendil/anaconda3/envs/phd/lib/python3.7/site-packages/tqdm/std.py:668: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n",
      "  from pandas import Panel\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import urllib.request\n",
    "import urllib.parse\n",
    "import plotly.express as px\n",
    "import plotly.io as pio\n",
    "from sentence_transformers import CrossEncoder\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "from neo4j import GraphDatabase\n",
    "from matplotlib import pyplot\n",
    "import seaborn as sns\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import classification_report\n",
    "tqdm.pandas()\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "model1 = SentenceTransformer('lighteternal/stsb-xlm-r-greek-transfer')\n",
    "\n",
    "model2 = CrossEncoder('lighteternal/nli-xlm-r-greek')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Greek FEVER subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-01T11:58:40.899618Z",
     "start_time": "2022-02-01T11:58:40.863086Z"
    }
   },
   "outputs": [],
   "source": [
    "df_gr = pd.pandas.read_excel(\n",
    "    \"greek_fever_benchmark.xlsx\", engine='openpyxl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-01T11:58:53.747846Z",
     "start_time": "2022-02-01T11:58:53.487605Z"
    }
   },
   "outputs": [],
   "source": [
    "df_gr = df_gr.replace('\\n',' ', regex=True)\n",
    "df_gr = df_gr.replace('&nbsp;',' ', regex=True)\n",
    "df_gr = df_gr.replace('&amp;',' ', regex=True)\n",
    "print(len(df_gr))\n",
    "\n",
    "\n",
    "df_gr[\"evidence_tok\"] =  df_gr['evidence_gr'].progress_apply(lambda x: sent_tokenize(x) if pd.isna(x)==False else '-')\n",
    "df_gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-01T11:59:00.821167Z",
     "start_time": "2022-02-01T11:59:00.815849Z"
    }
   },
   "outputs": [],
   "source": [
    "df_gr_tok = df_gr.explode('evidence_tok')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-01T11:48:23.903818Z",
     "start_time": "2022-02-01T11:48:23.792748Z"
    }
   },
   "outputs": [],
   "source": [
    "df_gr_tok.to_csv(\"./fever_dataset_tok.csv\", sep = ',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Populating the Neo4j with Wikipedia data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-01T11:48:23.999436Z",
     "start_time": "2022-02-01T11:48:23.996142Z"
    }
   },
   "outputs": [],
   "source": [
    "driver = GraphDatabase.driver(\n",
    "    'bolt://localhost:11005', auth=('neo4j', 'password'))\n",
    "\n",
    "\n",
    "def run_query(query, params={}):\n",
    "    with driver.session(database='fevergr') as session:\n",
    "        result = session.run(query, params)\n",
    "        return pd.DataFrame([r.values() for r in result], columns=result.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-01T11:41:50.987212Z",
     "start_time": "2022-02-01T11:41:46.969285Z"
    }
   },
   "outputs": [],
   "source": [
    "run_query(\"\"\"\n",
    "\n",
    "    USING PERIODIC COMMIT 1000\n",
    "    LOAD CSV WITH HEADERS FROM 'http://localhost:11001/project-dd090feb-9867-4887-9be2-93af8aeec75f/fever_dataset_tok.csv' AS row\n",
    "    MERGE (a:Article {title: coalesce(row.Column1, ' '), maintext: row.evidence_gr})\n",
    "    WITH a, row\n",
    "    UNWIND row.evidence_tok AS section\n",
    "    WITH a, section\n",
    "    MERGE (s:Section {name: section})\n",
    "    MERGE (a)-[r:HAS_SECTION]->(s)\n",
    "\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-31T18:26:08.396458Z",
     "start_time": "2022-01-31T18:26:08.383515Z"
    }
   },
   "outputs": [],
   "source": [
    "run_query(\n",
    "    \"CREATE CONSTRAINT IF NOT EXISTS ON (e:Entity) ASSERT e.wikiDataItemId is UNIQUE;\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-31T19:11:09.135133Z",
     "start_time": "2022-01-31T18:26:09.739200Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "user_key = \"YOUR JSI WIKIFIER API KEY\"\n",
    "\n",
    "run_query(\"\"\"\n",
    "CALL apoc.periodic.iterate('\n",
    " MATCH (s:Section) RETURN s\n",
    " ','\n",
    " WITH s, \"http://wikifier.org/annotate-article?\" +\n",
    "        \"text=\" + apoc.text.urlencode(s.name) + \"&\" +\n",
    "        \"lang=auto&\" +\n",
    "        \"pageRankSqThreshold=0.99&\" +\n",
    "        \"applyPageRankSqThreshold=true&\" +\n",
    "        \"maxMentionEntropy=3&\" +\n",
    "        \"wikiDataClasses=false&\" +\n",
    "        \"wikiDataClassIds=false&\" +\n",
    "        \"userKey=\" + $userKey as url\n",
    "CALL apoc.load.json(url) YIELD value\n",
    "UNWIND value.annotations as annotation\n",
    "MERGE (e:Entity{wikiDataItemId:annotation.wikiDataItemId})\n",
    "ON CREATE SET e.title = annotation.title, e.url = annotation.url\n",
    "MERGE (s)-[:HAS_ENTITY]->(e)',\n",
    "{batchSize:8, params: {userKey:$user_key}})\n",
    "\"\"\", {\"user_key\": user_key})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-31T19:23:33.487066Z",
     "start_time": "2022-01-31T19:11:09.195829Z"
    }
   },
   "outputs": [],
   "source": [
    "run_query(\"\"\"\n",
    "// Iterate over entities\n",
    "MATCH (e:Entity)\n",
    "// Prepare a SparQL query\n",
    "WITH 'SELECT *\n",
    "      WHERE{\n",
    "        ?item rdfs:label ?name .\n",
    "        filter (?item = wd:' + e.wikiDataItemId + ')\n",
    "        filter (lang(?name) = \"en\" ) .\n",
    "      OPTIONAL{\n",
    "        ?item wdt:P31 [rdfs:label ?class] .\n",
    "        filter (lang(?class)=\"en\")\n",
    "      }}' AS sparql, e\n",
    "// make a request to Wikidata\n",
    "CALL apoc.load.jsonParams(\n",
    "    \"https://query.wikidata.org/sparql?query=\" + \n",
    "    apoc.text.urlencode(sparql),\n",
    "     { Accept: \"application/sparql-results+json\"}, null)\n",
    "YIELD value\n",
    "UNWIND value['results']['bindings'] as row\n",
    "FOREACH(ignoreme in case when row['class'] is not null then [1] else [] end | \n",
    "        MERGE (c:Class{name:row['class']['value']})\n",
    "        MERGE (e)-[:INSTANCE_OF]->(c));    \n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-31T19:23:33.585397Z",
     "start_time": "2022-01-31T19:23:33.544620Z"
    }
   },
   "outputs": [],
   "source": [
    "run_query(\"\"\"\n",
    "MATCH (e:Entity)-[:INSTANCE_OF]->(c:Class)\n",
    "WHERE c.name in [\"άνθρωπος\", \"human\"]\n",
    "SET e:Person;\n",
    "\"\"\")\n",
    "run_query(\"\"\"\n",
    "MATCH (e:Entity)-[:INSTANCE_OF]->(c:Class)\n",
    "WHERE c.name in [\"επιχείρηση\", \"οργανισμός\",\"εταιρεία\", \"business\", \"corporation\", \"organization\", \"company\"]\n",
    "SET e:Business;\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-31T19:23:41.616998Z",
     "start_time": "2022-01-31T19:23:33.639307Z"
    }
   },
   "outputs": [],
   "source": [
    "run_query(\"\"\"\n",
    "MATCH (e:Business)\n",
    "// Prepare a SparQL query\n",
    "WITH 'SELECT *\n",
    "      WHERE{\n",
    "        ?item rdfs:label ?name .\n",
    "        filter (?item = wd:' + e.wikiDataItemId + ')\n",
    "        filter (lang(?name) = \"auto\" ) .\n",
    "      OPTIONAL{\n",
    "        ?item wdt:P452 [rdfs:label ?industry] .\n",
    "        filter (lang(?industry)=\"auto\")\n",
    "      }}' AS sparql, e\n",
    "// make a request to Wikidata\n",
    "CALL apoc.load.jsonParams(\n",
    "    \"https://query.wikidata.org/sparql?query=\" + \n",
    "    apoc.text.urlencode(sparql),\n",
    "     { Accept: \"application/sparql-results+json\"}, null)\n",
    "YIELD value\n",
    "UNWIND value['results']['bindings'] as row\n",
    "FOREACH(ignoreme in case when row['industry'] is not null then [1] else [] end | \n",
    "        MERGE (i:Industry{name:row['industry']['value']})\n",
    "        MERGE (e)-[:PART_OF_INDUSTRY]->(i));\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-31T19:23:41.716307Z",
     "start_time": "2022-01-31T19:23:41.677651Z"
    }
   },
   "outputs": [],
   "source": [
    "run_query(\"\"\"\n",
    "MATCH (e:Entity)-[:INSTANCE_OF]->(c:Class)\n",
    "WHERE c.name in [\"city\", \"πόλη\"]\n",
    "SET e:City;\n",
    "\"\"\")\n",
    "\n",
    "run_query(\"\"\"\n",
    "MATCH (e:Entity)-[:INSTANCE_OF]->(c:Class)\n",
    "WHERE c.name in [\"χώρα\", \"country\"]\n",
    "SET e:Country;\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-01T11:48:26.355739Z",
     "start_time": "2022-02-01T11:48:26.342896Z"
    }
   },
   "outputs": [],
   "source": [
    "# Call Wikifier on hypothesis\n",
    "\n",
    "# import urllib.parse, urllib.request, json\n",
    "\n",
    "def CallWikifier(text, lang=\"el\", threshold=0.99):\n",
    "    # Prepare the URL.\n",
    "    data = urllib.parse.urlencode([\n",
    "        (\"text\", text), (\"auto\", lang),\n",
    "        (\"secondaryAnnotLanguage\", \"en\"),\n",
    "        (\"userKey\", \"unyifgroezsfzcbwhaprdfjhxskvgx\"),\n",
    "        (\"pageRankSqThreshold\", \"%g\" %\n",
    "         threshold), (\"applyPageRankSqThreshold\", \"true\"),\n",
    "        (\"wikiDataClasses\", \"true\"), (\"wikiDataClassIds\", \"false\"),\n",
    "        (\"support\", \"false\"), (\"ranges\", \"false\"),\n",
    "        (\"includeCosines\", \"false\"), (\"maxMentionEntropy\", \"3\")\n",
    "    ])\n",
    "    url = \"http://www.wikifier.org/annotate-article\"\n",
    "    # Call the Wikifier and read the response.\n",
    "    req = urllib.request.Request(url, data=data.encode(\"utf8\"), method=\"POST\")\n",
    "    with urllib.request.urlopen(req, timeout=120) as f:\n",
    "        response = f.read()\n",
    "        response = json.loads(response.decode(\"utf8\"))\n",
    "    # Output the annotations.\n",
    "    for annotation in response[\"annotations\"]:\n",
    "        print(\"%s (%s)\" % (annotation[\"title\"], annotation[\"wikiDataItemId\"]))\n",
    "    return response[\"annotations\"]\n",
    "\n",
    "\n",
    "def softmax(x):\n",
    "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running FEVER subset benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-01T17:25:39.215466Z",
     "start_time": "2022-02-01T16:57:18.342148Z"
    }
   },
   "outputs": [],
   "source": [
    "# Get all claims from greek_fever_beenchmark:\n",
    "claims = df_gr['claim_gr']\n",
    "claims.dropna(inplace=True)\n",
    "print(f\"Number of claims:{len(claims)}\")\n",
    "labels_farfetched = []\n",
    "\n",
    "for claim in claims:\n",
    "    # Wikify claim\n",
    "    print('===============================')\n",
    "    print(f'Processing claim: {claim}')\n",
    "    hypo_list = CallWikifier(claim)\n",
    "\n",
    "    # Shortest path between found entities in Neo4j graph\n",
    "    hypo_ents = []\n",
    "    print('Searching for entities in claim...')\n",
    "    for item in hypo_list:\n",
    "        hypo_ents.append(item[\"wikiDataItemId\"])\n",
    "        # print(item[\"wikiDataItemId\"])\n",
    "    if len(hypo_ents) > 0:\n",
    "        path_count = (len(hypo_ents)-1)*2\n",
    "        #print('Discovered entities in claim: ', hypo_ents)\n",
    "        print('Checking chains of entity mentions, maximum possible length of path: ', path_count)\n",
    "        \n",
    "        # Evidence constructor on found entities\n",
    "        query = \"\"\"\n",
    "        MATCH (n:Entity) WHERE n.wikiDataItemId IN $hypo_ents\n",
    "        WITH collect(n) as nodes\n",
    "        UNWIND nodes as n\n",
    "        UNWIND nodes as m\n",
    "        \n",
    "        WITH nodes, n, m WHERE id(n) > id(m)\n",
    "         MATCH path =( (n)-[*0..PATHCOUNT]-(m) ) WHERE NONE(k IN nodes(path) WHERE k:Class)\n",
    "            WITH nodes, path WHERE ALL(n IN nodes WHERE n IN nodes(path))\n",
    "\n",
    "        RETURN nodes(path), length(path) ORDER BY length(path) DESC LIMIT 1000\n",
    "        \"\"\"\n",
    "\n",
    "        query_upd = (query.replace(\"PATHCOUNT\", str(min(path_count, 3))))\n",
    "\n",
    "        # shortestPath for multiple nodes:\n",
    "        result = run_query(query_upd, {\"hypo_ents\": hypo_ents})\n",
    "        \n",
    "        # Also checking individual entity mentions\n",
    "        print(f'Found {len(result)} chained entity mentions.')\n",
    "        if len(result) < 1:\n",
    "            print(\"Checking individual entities mentions...\")\n",
    "            query_nopath = \"\"\"\n",
    "                MATCH (n:Entity) WHERE n.wikiDataItemId IN $hypo_ents\n",
    "                WITH collect(n) as nodes\n",
    "                UNWIND nodes as n\n",
    "                MATCH p=(n)<-[r:HAS_ENTITY]-(s:Section) \n",
    "                RETURN s.name\n",
    "                \"\"\"\n",
    "            result = run_query(query_nopath, {\"hypo_ents\": hypo_ents})\n",
    "            print(\n",
    "                f\"Found {len(result)} individual entity mentions on knowledge base\")\n",
    "\n",
    "        # Searching for premises with relevant entities\n",
    "        query_upd = query\n",
    "        candidate_premises = []\n",
    "        temp = \"\"\n",
    "        try:\n",
    "            for path in result['nodes(path)']:\n",
    "                for sent in path:\n",
    "                    if sent['name'] is not None:\n",
    "                        # print(sent['name'])\n",
    "                        temp += sent['name']+' '\n",
    "                candidate_premises.append(temp)\n",
    "                temp = \"\"\n",
    "                # print('--')\n",
    "\n",
    "        except:\n",
    "            for sent in result['s.name']:\n",
    "                # print(sent)\n",
    "                candidate_premises.append(sent)\n",
    "                temp = \"\"\n",
    "                # print('--')\n",
    "\n",
    "        if len(candidate_premises) > 0:\n",
    "            # Select best candidate evidence based on STS\n",
    "            hypo_emb = model1.encode(claim, convert_to_tensor=True)\n",
    "            cand_prem_emb = model1.encode(\n",
    "                candidate_premises, convert_to_tensor=True)\n",
    "\n",
    "            # Compute cosine-similarities\n",
    "            from sentence_transformers import util\n",
    "            cosine_scores = util.pytorch_cos_sim(hypo_emb, cand_prem_emb)\n",
    "            # cosine_scores\n",
    "\n",
    "            pd. set_option('display.max_columns', None)\n",
    "            pd. set_option('display.max_rows', None)\n",
    "            pd. set_option('display.max_colwidth', None)\n",
    "\n",
    "            df = pd.DataFrame()\n",
    "            df['candidates'] = candidate_premises\n",
    "            df['similarities'] = pd.DataFrame(\n",
    "                cosine_scores.cpu().numpy().transpose())\n",
    "            df.drop_duplicates(inplace=True)\n",
    "            df.sort_values(by='similarities', ascending=False)\n",
    "\n",
    "            premise = candidate_premises[int(torch.argmax(cosine_scores))]\n",
    "\n",
    "            # Most relevant premise comparison\n",
    "            scores = model2.predict([(premise, claim)])\n",
    "\n",
    "            print(\"\\n Most relevant premise: \", premise)\n",
    "            print(\"---\")\n",
    "            #print(\"Claim: \",claim)\n",
    "            # Convert scores to labels\n",
    "            label_mapping = ['REFUTES', 'SUPPORTS', 'NOT ENOUGH INFO']\n",
    "            labels = [label_mapping[score_max]\n",
    "                      for score_max in scores.argmax(axis=1)]\n",
    "            print(\"Score C/E/N: \", scores, labels)\n",
    "            labels_farfetched.append(\"\".join(labels))\n",
    "        else:\n",
    "            print('No relevant premises found in knowledge base! Not enough info to conduct claim verification based on existing data')\n",
    "            labels_farfetched.append('NOT ENOUGH INFO')\n",
    "\n",
    "    else:\n",
    "        print('No entities matched in claim! Not enough info to conduct claim verification based on existing data')\n",
    "        labels_farfetched.append('NOT ENOUGH INFO')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-01T20:28:27.090890Z",
     "start_time": "2022-02-01T20:28:27.085768Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 precision    recall  f1-score   support\n",
      "\n",
      "NOT ENOUGH INFO       0.36      0.80      0.49        20\n",
      "        REFUTES       0.91      0.72      0.80        93\n",
      "       SUPPORTS       0.84      0.70      0.76        37\n",
      "\n",
      "       accuracy                           0.73       150\n",
      "      macro avg       0.70      0.74      0.69       150\n",
      "   weighted avg       0.82      0.73      0.75       150\n",
      "\n"
     ]
    }
   ],
   "source": [
    "classification_report = classification_report(df_gr['true_result'], labels_farfetched)\n",
    "print(classification_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "phd",
   "language": "python",
   "name": "phd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "toc-showcode": false,
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "144.844px",
    "left": "2048.56px",
    "right": "20px",
    "top": "171px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
