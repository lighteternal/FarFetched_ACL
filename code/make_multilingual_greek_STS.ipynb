{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22bd9b84",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-20T17:00:34.395473Z",
     "start_time": "2021-09-20T17:00:34.317918Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This script contains an example how to extend an existent sentence embedding model to new languages.\n",
    "\n",
    "Given a (monolingual) teacher model you would like to extend to new languages, which is specified in the teacher_model_name\n",
    "variable. We train a multilingual student model to imitate the teacher model (variable student_model_name)\n",
    "on multiple languages.\n",
    "\n",
    "For training, you need parallel sentence data (machine translation training data). You need tab-seperated files (.tsv)\n",
    "with the first column a sentence in a language understood by the teacher model, e.g. English,\n",
    "and the further columns contain the according translations for languages you want to extend to.\n",
    "\n",
    "This scripts downloads automatically the TED2020 corpus: https://github.com/UKPLab/sentence-transformers/blob/master/docs/datasets/TED2020.md\n",
    "This corpus contains transcripts from\n",
    "TED and TEDx talks, translated to 100+ languages. For other parallel data, see get_parallel_data_[].py scripts\n",
    "\n",
    "Further information can be found in our paper:\n",
    "Making Monolingual Sentence Embeddings Multilingual using Knowledge Distillation\n",
    "https://arxiv.org/abs/2004.09813\n",
    "\"\"\"\n",
    "\n",
    "import mlnotify\n",
    "\n",
    "from sentence_transformers import SentenceTransformer, LoggingHandler, models, evaluation, losses\n",
    "from torch.utils.data import DataLoader\n",
    "from sentence_transformers.datasets import ParallelSentencesDataset\n",
    "from datetime import datetime\n",
    "\n",
    "import os\n",
    "import logging\n",
    "import sentence_transformers.util\n",
    "import csv\n",
    "import gzip\n",
    "from tqdm.autonotebook import tqdm\n",
    "import numpy as np\n",
    "import zipfile\n",
    "import io\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s - %(message)s',\n",
    "                    datefmt='%Y-%m-%d %H:%M:%S',\n",
    "                    level=logging.INFO,\n",
    "                    handlers=[LoggingHandler()])\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "teacher_model_name = 'cross-encoder/nli-deberta-base'   #Our monolingual teacher model, we want to convert to multiple languages\n",
    "student_model_name = 'xlm-roberta-base'       #Multilingual base model we use to imitate the teacher model\n",
    "\n",
    "max_seq_length = 400                #Student model max. lengths for inputs (number of word pieces)\n",
    "train_batch_size = 16               #Batch size for training\n",
    "inference_batch_size = 64          #Batch size at inference\n",
    "max_sentences_per_language = 50000000 #Maximum number of  parallel sentences for training\n",
    "train_max_sentence_length = 400     #Maximum length (characters) for parallel training sentences\n",
    "\n",
    "num_epochs = 4                      #Train for x epochs\n",
    "num_warmup_steps = 10000             #Warumup steps\n",
    "\n",
    "num_evaluation_steps = 1000          #Evaluate performance after every xxxx steps\n",
    "dev_sentences = 1000                 #Number of parallel sentences to be used for development\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0af1eb3b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-12T14:35:31.409131Z",
     "start_time": "2021-09-12T14:35:31.396012Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define the language codes you would like to extend the model to\n",
    "source_languages = set(['en'])                      # Our teacher model accepts English (en) sentences\n",
    "target_languages = set(['el'])    # We want to extend the model to these new languages. For language codes, see the header of the train file\n",
    "\n",
    "\n",
    "output_path = \"output/make-multilingual-\"+\"-\".join(sorted(list(source_languages))+sorted(list(target_languages)))+\"-\"+datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "\n",
    "\n",
    "# This function downloads a corpus if it does not exist\n",
    "def download_corpora(filepaths):\n",
    "    if not isinstance(filepaths, list):\n",
    "        filepaths = [filepaths]\n",
    "\n",
    "    for filepath in filepaths:\n",
    "        if not os.path.exists(filepath):\n",
    "            print(filepath, \"does not exists. Try to download from server\")\n",
    "            filename = os.path.basename(filepath)\n",
    "            url = \"https://sbert.net/datasets/\" + filename\n",
    "            sentence_transformers.util.http_get(url, filepath)\n",
    "\n",
    "\n",
    "# Here we define train train and dev corpora\n",
    "train_corpus = \"datasets/ted2020.tsv.gz\"         # Transcripts of TED talks, crawled 2020\n",
    "sts_corpus = \"datasets/STS2017-extended.zip\"     # Extended STS2017 dataset for more languages\n",
    "parallel_sentences_folder = \"parallel-sentences/\"\n",
    "\n",
    "# Check if the file exists. If not, they are downloaded\n",
    "download_corpora([train_corpus, sts_corpus])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4491776",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-12T14:35:31.601887Z",
     "start_time": "2021-09-12T14:35:31.594737Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Create parallel files for the selected language combinations\n",
    "os.makedirs(parallel_sentences_folder, exist_ok=True)\n",
    "train_files = []\n",
    "dev_files = []\n",
    "files_to_create = []\n",
    "for source_lang in source_languages:\n",
    "    for target_lang in target_languages:\n",
    "#         output_filename_train = os.path.join(parallel_sentences_folder, \"TED2020-{}-{}-train.tsv.gz\".format(source_lang, target_lang))\n",
    "#         output_filename_dev = os.path.join(parallel_sentences_folder, \"TED2020-{}-{}-dev.tsv.gz\".format(source_lang, target_lang))\n",
    "        output_filename_train = os.path.join(parallel_sentences_folder, \"combined-{}-{}-train.tsv.gz\".format(source_lang, target_lang))\n",
    "        output_filename_dev = os.path.join(parallel_sentences_folder, \"combined-{}-{}-dev.tsv.gz\".format(source_lang, target_lang))\n",
    "        train_files.append(output_filename_train)\n",
    "        dev_files.append(output_filename_dev)\n",
    "        if not os.path.exists(output_filename_train) or not os.path.exists(output_filename_dev):\n",
    "            files_to_create.append({'src_lang': source_lang, 'trg_lang': target_lang,\n",
    "                                    'fTrain': gzip.open(output_filename_train, 'wt', encoding='utf8'),\n",
    "                                    'fDev': gzip.open(output_filename_dev, 'wt', encoding='utf8'),\n",
    "                                    'devCount': 0\n",
    "                                    })\n",
    "\n",
    "if len(files_to_create) > 0:\n",
    "    print(\"Parallel sentences files {} do not exist. Create these files now\".format(\", \".join(map(lambda x: x['src_lang']+\"-\"+x['trg_lang'], files_to_create))))\n",
    "    with gzip.open(train_corpus, 'rt', encoding='utf8') as fIn:\n",
    "        reader = csv.DictReader(fIn, delimiter='\\t', quoting=csv.QUOTE_NONE)\n",
    "        for line in tqdm(reader, desc=\"Sentences\"):\n",
    "            for outfile in files_to_create:\n",
    "                src_text = line[outfile['src_lang']].strip()\n",
    "                trg_text = line[outfile['trg_lang']].strip()\n",
    "\n",
    "                if src_text != \"\" and trg_text != \"\":\n",
    "                    if outfile['devCount'] < dev_sentences:\n",
    "                        outfile['devCount'] += 1\n",
    "                        fOut = outfile['fDev']\n",
    "                    else:\n",
    "                        fOut = outfile['fTrain']\n",
    "\n",
    "                    fOut.write(\"{}\\t{}\\n\".format(src_text, trg_text))\n",
    "\n",
    "    for outfile in files_to_create:\n",
    "        outfile['fTrain'].close()\n",
    "        outfile['fDev'].close()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cceb96cf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-12T14:35:48.854021Z",
     "start_time": "2021-09-12T14:35:32.195057Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "######## Start the extension of the teacher model to multiple languages ########\n",
    "logger.info(\"Load teacher model\")\n",
    "teacher_model = SentenceTransformer(teacher_model_name)\n",
    "\n",
    "\n",
    "logger.info(\"Create student model from scratch\")\n",
    "word_embedding_model = models.Transformer(student_model_name, max_seq_length=max_seq_length)\n",
    "# Apply mean pooling to get one fixed sized sentence vector\n",
    "pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension())\n",
    "student_model = SentenceTransformer(modules=[word_embedding_model, pooling_model])\n",
    "\n",
    "\n",
    "###### Read Parallel Sentences Dataset ######\n",
    "train_data = ParallelSentencesDataset(student_model=student_model, teacher_model=teacher_model, batch_size=inference_batch_size, use_embedding_cache=True)\n",
    "for train_file in train_files:\n",
    "    train_data.load_data(train_file, max_sentences=max_sentences_per_language, max_sentence_length=train_max_sentence_length)\n",
    "\n",
    "train_dataloader = DataLoader(train_data, shuffle=True, batch_size=train_batch_size)\n",
    "train_loss = losses.MSELoss(model=student_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39089850",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-12T14:35:51.585626Z",
     "start_time": "2021-09-12T14:35:48.948580Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "#### Evaluate cross-lingual performance on different tasks #####\n",
    "evaluators = []         #evaluators has a list of different evaluator classes we call periodically\n",
    "\n",
    "for dev_file in dev_files:\n",
    "    logger.info(\"Create evaluator for \" + dev_file)\n",
    "    src_sentences = []\n",
    "    trg_sentences = []\n",
    "    with gzip.open(dev_file, 'rt', encoding='utf8') as fIn:\n",
    "        for line in fIn:\n",
    "            splits = line.strip().split('\\t')\n",
    "            if splits[0] != \"\" and splits[1] != \"\":\n",
    "                src_sentences.append(splits[0])\n",
    "                trg_sentences.append(splits[1])\n",
    "\n",
    "\n",
    "    #Mean Squared Error (MSE) measures the (euclidean) distance between teacher and student embeddings\n",
    "    dev_mse = evaluation.MSEEvaluator(src_sentences, trg_sentences, name=os.path.basename(dev_file), teacher_model=teacher_model, batch_size=inference_batch_size)\n",
    "    evaluators.append(dev_mse)\n",
    "\n",
    "    # TranslationEvaluator computes the embeddings for all parallel sentences. It then check if the embedding of source[i] is the closest to target[i] out of all available target sentences\n",
    "    dev_trans_acc = evaluation.TranslationEvaluator(src_sentences, trg_sentences, name=os.path.basename(dev_file),batch_size=inference_batch_size)\n",
    "    evaluators.append(dev_trans_acc)\n",
    "\n",
    "\n",
    "##### Read cross-lingual Semantic Textual Similarity (STS) data ####\n",
    "all_languages = list(set(list(source_languages)+list(target_languages)))\n",
    "sts_data = {}\n",
    "\n",
    "#Open the ZIP File of STS2017-extended.zip and check for which language combinations we have STS data\n",
    "with zipfile.ZipFile(sts_corpus) as zip:\n",
    "    filelist = zip.namelist()\n",
    "    sts_files = []\n",
    "\n",
    "    for i in range(len(all_languages)):\n",
    "        for j in range(i, len(all_languages)):\n",
    "            lang1 = all_languages[i]\n",
    "            lang2 = all_languages[j]\n",
    "            filepath = 'STS2017-extended/STS.en-el.txt'.format(lang1, lang2)\n",
    "            if filepath not in filelist:\n",
    "                lang1, lang2 = lang2, lang1\n",
    "                filepath = 'STS2017-extended/STS.en-el.txt'.format(lang1, lang2)\n",
    "\n",
    "            if filepath in filelist:\n",
    "                filename = os.path.basename(filepath)\n",
    "                sts_data[filename] = {'sentences1': [], 'sentences2': [], 'scores': []}\n",
    "\n",
    "                fIn = zip.open(filepath)\n",
    "                for line in io.TextIOWrapper(fIn, 'utf8'):\n",
    "                    sent1, sent2, score = line.strip().split(\"\\t\")\n",
    "                    score = float(score)\n",
    "                    sts_data[filename]['sentences1'].append(sent1)\n",
    "                    sts_data[filename]['sentences2'].append(sent2)\n",
    "                    sts_data[filename]['scores'].append(score)\n",
    "\n",
    "for filename, data in sts_data.items():\n",
    "    test_evaluator = evaluation.EmbeddingSimilarityEvaluator(data['sentences1'], data['sentences2'], data['scores'], batch_size=inference_batch_size, name=filename, show_progress_bar=False)\n",
    "    evaluators.append(test_evaluator)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "970b4c18",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-20T16:55:47.541173Z",
     "start_time": "2021-09-20T16:55:42.973390Z"
    }
   },
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "model = SentenceTransformer('/home/earendil/Desktop/ML_playground/sentence-transformers/examples/training/multilingual/output/make-multilingual-en-el-2021-09-11_23-07-33')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "055c3db7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-12T13:34:06.860238Z",
     "start_time": "2021-09-12T13:34:06.629963Z"
    }
   },
   "outputs": [],
   "source": [
    "sentences1 = ['Είναι ένας χαρούμενος άντρας.',\n",
    "             'Είναι ένας χαρούμενος άντρας.',\n",
    "             'Είναι ένας χαρούμενος άντρας.']\n",
    "\n",
    "sentences2 = [\"Το σκυλάκι είναι χαρούμενο.\",\n",
    "             \"Ο άνθρωπος είναι πολύ ευτυχισμένος.\",\n",
    "             \"Είναι μια όμορφη μέρα.\"]\n",
    "\n",
    "embeddings1 = model.encode(sentences1, convert_to_tensor=True)\n",
    "embeddings2 = model.encode(sentences2, convert_to_tensor=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3588b0b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-12T13:34:07.535821Z",
     "start_time": "2021-09-12T13:34:07.522896Z"
    }
   },
   "outputs": [],
   "source": [
    "#Compute cosine-similarits\n",
    "from sentence_transformers import util\n",
    "cosine_scores = util.pytorch_cos_sim(embeddings1, embeddings2)\n",
    "\n",
    "#Output the pairs with their score\n",
    "for i in range(len(sentences1)):\n",
    "    print(\"{} \\t\\t {} \\t\\t Score: {:.4f}\".format(sentences1[i], sentences2[i], cosine_scores[i][i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "105574fa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-20T16:55:50.347940Z",
     "start_time": "2021-09-20T16:55:47.774063Z"
    }
   },
   "outputs": [],
   "source": [
    "sentences1 = ['Το κινητό έπεσε και έσπασε.',\n",
    "             'Το κινητό έπεσε και έσπασε.',\n",
    "             'Το κινητό έπεσε και έσπασε.']\n",
    "\n",
    "sentences2 = [\"H πτώση κατέστρεψε τη συσκευή.\",\n",
    "             \"Το αυτοκίνητο έσπασε στα δυο.\",\n",
    "             \"Ο υπουργός έπεσε και έσπασε το πόδι του.\"]\n",
    "\n",
    "embeddings1 = model.encode(sentences1, convert_to_tensor=True)\n",
    "embeddings2 = model.encode(sentences2, convert_to_tensor=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed889ab",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-20T16:55:50.925009Z",
     "start_time": "2021-09-20T16:55:50.913891Z"
    }
   },
   "outputs": [],
   "source": [
    "#Compute cosine-similarities (clone the sentence-trasformers repo first)\n",
    "from sentence_transformers import util\n",
    "cosine_scores = util.pytorch_cos_sim(embeddings1, embeddings2)\n",
    "\n",
    "#Output the pairs with their score\n",
    "for i in range(len(sentences1)):\n",
    "    print(\"{} \\t\\t {} \\t\\t Score: {:.4f}\".format(sentences1[i], sentences2[i], cosine_scores[i][i]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda110",
   "language": "python",
   "name": "cuda110"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
